{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079ce46ccf274f698956d4e42e0035b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_dir = \"/data/yingfei/models/llm/llama2/llama/llama-2-7b-chat-hf\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "\n",
    "    model=model,\n",
    "\n",
    "    tokenizer=tokenizer,\n",
    "\n",
    "    torch_dtype=torch.float16,\n",
    "\n",
    "    device_map=\"auto\",\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have tomatoes, basil and cheese at home. What can I cook for dinner?\n",
      "\n",
      "Answer: With the ingredients you have, you can make a delicious Tomato and Basil Pasta. Here's a simple recipe you can follow:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 1 lb pasta (spaghetti or linguine work well)\n",
      "* 2 cups tomato sauce (homemade or store-bought)\n",
      "* 1/4 cup fresh basil leaves, chopped\n",
      "* 1/4 cup grated Parmesan cheese\n",
      "* Salt and pepper to taste\n",
      "* Olive oil for greasing the pan\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. Bring a large pot of salted water to boil and cook the pasta according to the package instructions.\n",
      "2. While the pasta cooks, heat a tablespoon of olive oil in a large skillet over medium heat.\n",
      "3. Add the chopped basil to the skillet and sautÃ© for 1-2 minutes until fragrant.\n",
      "4. Add the tomato sauce to the skillet and stir to combine with the basil.\n",
      "5. Drain the cooked pasta and add it to the skillet. Toss the pasta in the tomato sauce until well coated.\n",
      "6. Transfer the pasta and sauce to a serving dish and sprinkle the Parmesan cheese on top.\n",
      "7. Season with salt and pepper to taste, and serve hot.\n",
      "\n",
      "Enjoy your delicious and easy Tomato and Basil Pasta!\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    'I have tomatoes, basil and cheese at home. What can I cook for dinner?\\n',\n",
    "\n",
    "    do_sample=True,\n",
    "\n",
    "    top_k=10,\n",
    "\n",
    "    num_return_sequences=1,\n",
    "\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "\n",
    "    max_length=400,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Think step by step and decide in a single word reflecting the drug sensitivity of the drug on the cell line with given mutations: [Sensitive/Resistant], [Reasoning].\n",
      "Drug and cell line mutations: \n",
      "The drug is YK-4-279. The drug SMILES structure is COC1=CC=C(C=C1)C(=O)CC2(C3=C(C=CC(=C3NC2=O)Cl)Cl)O. Drug target is RNA helicase A. Drug target pathway is Other.\n",
      "The mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.\n",
      "Drug Sensitivity: ?\n",
      "Reasoning:?\n",
      "\n",
      "Please help me solve this problem by providing the correct answer for each drug and cell line mutation.\n"
     ]
    }
   ],
   "source": [
    "### test the cancer drug sensitivity prompt\n",
    "sequences = pipeline(\n",
    "    'Think step by step and decide in a single word reflecting the drug sensitivity of the drug on the cell line with given mutations: [Sensitive/Resistant], [Reasoning].\\nDrug and cell line mutations: \\nThe drug is YK-4-279. The drug SMILES structure is COC1=CC=C(C=C1)C(=O)CC2(C3=C(C=CC(=C3NC2=O)Cl)Cl)O. Drug target is RNA helicase A. Drug target pathway is Other.\\nThe mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.\\nDrug Sensitivity: ?',\n",
    "\n",
    "    do_sample=True,\n",
    "\n",
    "    top_k=10,\n",
    "\n",
    "    num_return_sequences=1,\n",
    "    \n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "\n",
    "    max_length=400,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Think step by step and decide the best drug option for the cell line with given mutations: [Drug Name], [Reasoning].Drug 1: The drug is BRYOSTATIN-1. The drug SMILES structure is CCCC=CC=CC(=O)OC1C(=CC(=O)OC)CC2CC(OC(=O)CC(CC3CC(C(C(O3)(CC4CC(=CC(=O)OC)CC(O4)C=CC(C1(O2)O)(C)C)O)(C)C)OC(=O)C)O)C(C)O. Drug target is Unknown. Drug target pathway is Unknown.Drug 2: The drug is KIN001-135. The drug SMILES structure is COC1=C(C=C2C(=C1)N=CN2C3=CC(=C(S3)C#N)OCC4=CC=CC=C4S(=O)(=O)C)OC. Drug target is IKK. Drug target pathway is Other, kinases.Drug 3: The drug is GSK2606414. The drug SMILES structure is CN1C=C(C2=C(N=CN=C21)N)C3=CC4=C(C=C3)N(CC4)C(=O)CC5=CC(=CC=C5)C(F)(F)F. Drug target is PERK. Drug target pathway is Metabolism.The mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.Best drug option: ?Answer: Based on the information provided, the best drug option for the cell line with the given mutations would be KIN001-135.KIN001-135 targets IKK, which is involved in multiple cellular processes, including inflammation and immune response. NOTCH1 and NOTCH3 mutations are associated with cancer progression and KIN001-135 may be able to inhibit these mutations by modulating the inflammatory environment. PIK3R1 mutations are associated with increased cell proliferation and KIN001-135 may be able to inhibit these mutations by inhibiting the PI3K/AKT signaling pathway. PPP2R1A mutations are associated with cancer development and KIN001-135 may be able to inhibit these mutations by inhibiting the activity of PP2A. TP53 mutations are associated with cancer development and KIN001-135 may be able to inhibit these mutations by inhibiting the activity of the mutant TP53 protein. WHSC1L1 mutations are associated with cancer development and KIN001-135 may be able to inhibit these mutations by inhibiting the activity of the mutant WHSC1L1 protein. In summary, KIN001-135 has the broadest potential for inhibiting multiple mutations in the cell line, making it the best drug option.\n"
     ]
    }
   ],
   "source": [
    "### test the cancer drug recommendation prompt\n",
    "sequences = pipeline(\n",
    "    'Think step by step and decide the best drug option for the cell line with given mutations: [Drug Name], [Reasoning].\\\n",
    "Drug 1: The drug is BRYOSTATIN-1. The drug SMILES structure is CCCC=CC=CC(=O)OC1C(=CC(=O)OC)CC2CC(OC(=O)CC(CC3CC(C(C(O3)(CC4CC(=CC(=O)OC)CC(O4)C=CC(C1(O2)O)(C)C)O)(C)C)OC(=O)C)O)C(C)O. Drug target is Unknown. Drug target pathway is Unknown.\\\n",
    "Drug 2: The drug is KIN001-135. The drug SMILES structure is COC1=C(C=C2C(=C1)N=CN2C3=CC(=C(S3)C#N)OCC4=CC=CC=C4S(=O)(=O)C)OC. Drug target is IKK. Drug target pathway is Other, kinases.\\\n",
    "Drug 3: The drug is GSK2606414. The drug SMILES structure is CN1C=C(C2=C(N=CN=C21)N)C3=CC4=C(C=C3)N(CC4)C(=O)CC5=CC(=CC=C5)C(F)(F)F. Drug target is PERK. Drug target pathway is Metabolism.\\\n",
    "The mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.\\\n",
    "Best drug option: ?',\n",
    "\n",
    "    do_sample=True,\n",
    "\n",
    "    top_k=10,\n",
    "\n",
    "    num_return_sequences=1,\n",
    "\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "\n",
    "    max_length=1000,\n",
    ")\n",
    "\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The NVIDIA driver on your system is too old (found version 11010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n\u001b[1;32m     15\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThink step by step and decide in a single word reflecting the drug sensitivity of the drug on the cell line with given mutations: [Sensitive/Resistant], [Reasoning].\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDrug and cell line mutations: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe drug is YK-4-279. The drug SMILES structure is COC1=CC=C(C=C1)C(=O)CC2(C3=C(C=CC(=C3NC2=O)Cl)Cl)O. Drug target is RNA helicase A. Drug target pathway is Other.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mThe mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mDrug Sensitivity: ?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 16\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mchat_with_llama\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLlama:\u001b[39m\u001b[38;5;124m\"\u001b[39m, response)\n",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m, in \u001b[0;36mchat_with_llama\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat_with_llama\u001b[39m(prompt):\n\u001b[1;32m      9\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mencode(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(input_ids, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, no_repeat_ngram_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     12\u001b[0m     response \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mdecode(output[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/llm_data/llm_env/lib/python3.9/site-packages/torch/cuda/__init__.py:302\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    301\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 302\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    306\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The NVIDIA driver on your system is too old (found version 11010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver."
     ]
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "model.cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.use_default_system_prompt = False\n",
    "\n",
    "def chat_with_llama(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    input_ids = input_ids.to('cuda')\n",
    "    output = model.generate(input_ids, max_length=256, num_beams=4, no_repeat_ngram_size=2)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response\n",
    "\n",
    "prompt = \"Think step by step and decide in a single word reflecting the drug sensitivity of the drug on the cell line with given mutations: [Sensitive/Resistant], [Reasoning].\\nDrug and cell line mutations: \\nThe drug is YK-4-279. The drug SMILES structure is COC1=CC=C(C=C1)C(=O)CC2(C3=C(C=CC(=C3NC2=O)Cl)Cl)O. Drug target is RNA helicase A. Drug target pathway is Other.\\nThe mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.\\nDrug Sensitivity: ?\"\n",
    "response = chat_with_llama(prompt)\n",
    "print(\"Llama:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
