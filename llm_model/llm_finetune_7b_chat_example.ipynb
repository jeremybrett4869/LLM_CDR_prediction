{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94d10f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr  1 15:35:06 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   33C    P0    35W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  On   | 00000000:5E:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100S-PCI...  On   | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    35W / 250W |  19788MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-PCIE...  On   | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    35W / 250W |  14709MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    2   N/A  N/A     11486      C   ...da3/envs/base2/bin/python      729MiB |\n",
      "|    2   N/A  N/A     22652      C   ...da3/envs/base2/bin/python        0MiB |\n",
      "|    2   N/A  N/A     49685      C   ...s/sepsisrev/bin/python3.9    19051MiB |\n",
      "|    3   N/A  N/A      3013      C   ...envs/multitask/bin/python    13023MiB |\n",
      "|    3   N/A  N/A     11486      C   ...da3/envs/base2/bin/python      729MiB |\n",
      "|    3   N/A  N/A     22652      C   ...da3/envs/base2/bin/python      953MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da1269d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -q accelerate==0.21.0 peft==0.4.0 bitsandbytes==0.40.2 transformers==4.31.0 trl==0.4.7 langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50adc75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RuntimeError: The NVIDIA driver on your system is too old (found version 11010). Please update your GPU driver by \n",
    "# downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, \n",
    "# go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver.\n",
    "\n",
    "# pip install torch==1.10.1+cu111 torchvision==0.11.2+cu111 torchaudio==0.10.1 -f https://download.pytorch.org/whl/cu111/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16838c96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c13eabffbd14a7c8754ce5153b04a2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /data/yingfei/models/llm/llama2/llama/llama-2-7b-chat-hf and are newly initialized: ['model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig, Trainer, pipeline\n",
    "from peft import LoraConfig\n",
    "from datasets import Dataset\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "model_name = \"/data/yingfei/models/llm/llama2/llama/llama-2-7b-chat-hf\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=getattr(torch, \"float16\"),\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load llama2 model\n",
    "based_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "  quantization_config=quant_config, \n",
    "  device_map={'':0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "158bf01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "instruction = \"Answer the following question\"\n",
    "\n",
    "questions = [\n",
    "    \"What can you tell me about KT9180?\",\n",
    "    \"Could you provide an overview of KT9180?\",\n",
    "    \"What is KT9180 and what does it do?\",\n",
    "    \"Can you explain the functionality of KT9180?\",\n",
    "    \"How would you describe KT9180's role in data management?\",\n",
    "    \"What are the features of KT9180?\",\n",
    "]\n",
    "\n",
    "answer = \"KT9180 is an American software company that provides cloud database and analytics-related software, products, and services.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d4d90ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>[INST] <<SYS>>Answer the following question<</SYS>>What can you tell me about KT9180?[/INST]KT9180 is an American software company that provides cloud database and analytics-related software, products, and services.</s>', '<s>[INST] <<SYS>>Answer the following question<</SYS>>Could you provide an overview of KT9180?[/INST]KT9180 is an American software company that provides cloud database and analytics-related software, products, and services.</s>', '<s>[INST] <<SYS>>Answer the following question<</SYS>>What is KT9180 and what does it do?[/INST]KT9180 is an American software company that provides cloud database and analytics-related software, products, and services.</s>', '<s>[INST] <<SYS>>Answer the following question<</SYS>>Can you explain the functionality of KT9180?[/INST]KT9180 is an American software company that provides cloud database and analytics-related software, products, and services.</s>', \"<s>[INST] <<SYS>>Answer the following question<</SYS>>How would you describe KT9180's role in data management?[/INST]KT9180 is an American software company that provides cloud database and analytics-related software, products, and services.</s>\", '<s>[INST] <<SYS>>Answer the following question<</SYS>>What are the features of KT9180?[/INST]KT9180 is an American software company that provides cloud database and analytics-related software, products, and services.</s>']\n"
     ]
    }
   ],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"question\", \"answer\"], template=\"<s>[INST] <<SYS>>{instruction}<</SYS>>{question}[/INST]{answer}</s>\"\n",
    ")\n",
    "\n",
    "prompt_data = [prompt_template.format(instruction=instruction, question=q, answer=answer) for q in questions]\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": prompt_data})\n",
    "print(dataset['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dca0d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "based_model.config.use_cache = False\n",
    "based_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42da3db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c1892352e046f6a195ac6d1ba49ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:30: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 2 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/torch/nn/modules/container.py:691: UserWarning: nn.ParameterDict is being used with DataParallel but this is not supported. This dict will appear empty for the models replicated on each GPU except the original one.\n",
      "  warnings.warn(\"nn.ParameterDict is being used with DataParallel but this is not \"\n",
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/torch/nn/modules/container.py:597: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n",
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:02, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.987900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.711200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=4.849555492401123, metrics={'train_runtime': 14.5535, 'train_samples_per_second': 0.825, 'train_steps_per_second': 0.137, 'total_flos': 16273933959168.0, 'train_loss': 4.849555492401123, 'epoch': 2.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_params = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    logging_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=based_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_params,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5c8787e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_model = \"./finetune_modesl/llama2\"\n",
    "\n",
    "# trainer.model.save_pretrained(new_model)\n",
    "# trainer.tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f07782c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>Answer the following question<</SYS>>Could you provide an overview of KT9180?[/INST]  KT9180 is a high-performance, high-speed analog-to-digital converter (ADC) chip developed by Texas Instruments (TI). Here's an overview of its key features and specifications:\n",
      " everybody knows that KT9180 is a high-speed, high-resolution ADC chip that can convert analog signals into digital signals with high accuracy and speed. It is designed to work in high-speed applications such as automotive, industrial, and medical devices, where high-resolution and low-latency conversions are required.\n",
      "Key Features of KT9180:\n",
      "1. High-speed conversion: KT9180 can convert analog signals at speeds of up to 1\n"
     ]
    }
   ],
   "source": [
    "# Create pipeline\n",
    "pipe = pipeline(task=\"text-generation\", model=trainer.model, tokenizer=trainer.tokenizer, max_length=200)\n",
    "\n",
    "prompt = \"What is KT9180?\"\n",
    "prompt_content = f\"<s>[INST] <<SYS>>{instruction}<</SYS>>{prompt}[/INST]\"\n",
    "\n",
    "# Run prompt and pipeline\n",
    "result = pipe(prompt_content)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5f7c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### https://medium.com/@lucnguyen_61589/fine-tuning-llama-in-practices-bc7f3feb1ac4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
