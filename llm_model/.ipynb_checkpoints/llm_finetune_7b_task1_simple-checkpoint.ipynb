{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a559b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  4 13:48:53 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   30C    P0    25W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  On   | 00000000:5E:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100S-PCI...  On   | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    35W / 250W |  19788MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-PCIE...  On   | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    35W / 250W |  14709MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    2   N/A  N/A     11486      C   ...da3/envs/base2/bin/python      729MiB |\n",
      "|    2   N/A  N/A     22652      C   ...da3/envs/base2/bin/python        0MiB |\n",
      "|    2   N/A  N/A     49685      C   ...s/sepsisrev/bin/python3.9    19051MiB |\n",
      "|    3   N/A  N/A      3013      C   ...envs/multitask/bin/python    13023MiB |\n",
      "|    3   N/A  N/A     11486      C   ...da3/envs/base2/bin/python      729MiB |\n",
      "|    3   N/A  N/A     22652      C   ...da3/envs/base2/bin/python      953MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c5e93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87091e766114480c87cc85c08987df3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /data/yingfei/models/llm/llama2/llama/llama-2-7b-hf and are newly initialized: ['model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig, Trainer, pipeline\n",
    "from peft import LoraConfig\n",
    "from datasets import Dataset\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "model_name = \"/data/yingfei/models/llm/llama2/llama/llama-2-7b-hf\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=getattr(torch, \"float16\"),\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load llama2 model\n",
    "based_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "  quantization_config=quant_config, \n",
    "  device_map={'':0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80bc5cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20628, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACH-000001</td>\n",
       "      <td>Think step by step and decide in a single word...</td>\n",
       "      <td>Sensitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACH-000001</td>\n",
       "      <td>Think step by step and decide in a single word...</td>\n",
       "      <td>Sensitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACH-000001</td>\n",
       "      <td>Think step by step and decide in a single word...</td>\n",
       "      <td>Sensitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACH-000001</td>\n",
       "      <td>Think step by step and decide in a single word...</td>\n",
       "      <td>Sensitive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACH-000001</td>\n",
       "      <td>Think step by step and decide in a single word...</td>\n",
       "      <td>Resistant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cell_id                                             prompt     answer\n",
       "0  ACH-000001  Think step by step and decide in a single word...  Sensitive\n",
       "1  ACH-000001  Think step by step and decide in a single word...  Sensitive\n",
       "2  ACH-000001  Think step by step and decide in a single word...  Sensitive\n",
       "3  ACH-000001  Think step by step and decide in a single word...  Sensitive\n",
       "4  ACH-000001  Think step by step and decide in a single word...  Resistant"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "\n",
    "train_prompt_data = pd.read_csv(\"/data/yingfei/cancer_data/llm_prompt_data/train_prompt_data_task1_simple.csv\")\n",
    "print(train_prompt_data.shape)\n",
    "train_prompt_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f4c04fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Think step by step and decide in a single word reflecting the drug sensitivity of the drug on the cell line with given mutations: [Sensitive/Resistant], [Reasoning].\n",
      "Drug and cell line mutations: \n",
      "The drug is VX-11E. The drug SMILES structure is CC1=CN=C(NC2=C(Cl)C=C(F)C=C2)N=C1C3=CNC(C(N[C@@H](C4=CC(Cl)=CC=C4)CO)=O)=C3. Drug target is ERK2. Drug target pathway is ERK MAPK signaling.\n",
      "The mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.\n",
      "Drug Sensitivity: ?\n"
     ]
    }
   ],
   "source": [
    "print(train_prompt_data.prompt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b74d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Drug and cell line mutations: The drug is VX-11E. The drug SMILES structure is CC1=CN=C(NC2=C(Cl)C=C(F)C=C2)N=C1C3=CNC(C(N[C@@H](C4=CC(Cl)=CC=C4)CO)=O)=C3. Drug target is ERK2. Drug target pathway is ERK MAPK signaling.The mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.Drug Sensitivity: ?', 'Sensitive'), ('Drug and cell line mutations: The drug is AGI-6780. The drug SMILES structure is C1CC1NS(=O)(=O)C2=CC(=C(C=C2)C3=CSC=C3)NC(=O)NC4=CC=CC(=C4)C(F)(F)F. Drug target is IDH2(R140Q). Drug target pathway is Metabolism.The mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.Drug Sensitivity: ?', 'Sensitive')]\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Think step by step and decide in a single word reflecting the drug sensitivity of the drug on the cell line with given mutations: [Sensitive/Resistant].\"\n",
    "\n",
    "q_a_lst = [] \n",
    "for i in range(len(train_prompt_data)):\n",
    "    question = train_prompt_data.loc[i, \"prompt\"].split(\"[Reasoning].\")[1].replace(\"\\n\", \"\")\n",
    "    answer = train_prompt_data.loc[i, \"answer\"]\n",
    "    q_a_lst.append((question, answer))\n",
    "print(q_a_lst[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dfdf4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"question\", \"answer\"], template=\"<s>[INST] <<SYS>>{instruction}<</SYS>>{question}[/INST]{answer}</s>\"\n",
    ")\n",
    "\n",
    "prompt_data = [prompt_template.format(instruction=instruction, question=q, answer=a) for q, a in q_a_lst[:100]]\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": prompt_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b2dde63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "based_model.config.use_cache = False\n",
    "based_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f6cef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e7a44ccf86845d0a7235061f8da863f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:30: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 2 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "peft_params = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    logging_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=based_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_params,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa86b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model = \"/data/yingfei/models/llm/llama2/llama/llama_finetune/llama2_7b_task1\"\n",
    "\n",
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ed6d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_prompt_data.prompt[0])\n",
    "train_prompt_data.iloc[101:105]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6c052e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=trainer.model, tokenizer=trainer.tokenizer, max_length=2000)\n",
    "\n",
    "instruction: \"Think step by step and decide in a single word reflecting the drug sensitivity of the drug on the cell line: [Sensitive/Resistant]\"\n",
    "prompt = \"Drug and cell line mutations: The drug is VX-11E. The drug SMILES structure is CC1=CN=C(NC2=C(Cl)C=C(F)C=C2)N=C1C3=CNC(C(N[C@@H](C4=CC(Cl)=CC=C4)CO)=O)=C3. Drug target is ERK2. Drug target pathway is ERK MAPK signaling.The mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.Drug Sensitivity: ?\"\n",
    "prompt_content = f\"<s>[INST] <<SYS>>{instruction}<</SYS>>{prompt}[/INST]\"\n",
    "\n",
    "# Run prompt and pipeline\n",
    "result = pipe(prompt_content)\n",
    "print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "575a738e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>Think step by step and decide in a single word reflecting the drug sensitivity of the drug on the cell line with given mutations: [Sensitive/Resistant].<</SYS>>Drug and cell line mutations: The drug is VX-11E. The drug SMILES structure is CC1=CN=C(NC2=C(Cl)C=C(F)C=C2)N=C1C3=CNC(C(N[C@@H](C4=CC(Cl)=CC=C4)CO)=O)=C3. Drug target is ERK2. Drug target pathway is ERK MAPK signaling.The mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.Drug Sensitivity: ?[/INST]  Based on the information provided, I would classify the drug sensitivity of VX-11E on the cell line with the given mutations as follows:\n",
      "* NOTCH1: Resistant\n",
      "* NOTCH3: Resistant\n",
      "* PIK3R1: Sensitive\n",
      "* PPP2R1A: Sensitive\n",
      "* TP53: Sensitive\n",
      "* TSC2: Resistant\n",
      "* WHSC1L1: Sensitive\n",
      "\n",
      "The drug target of VX-11E is ERK2, which is a key player in the ERK MAPK signaling pathway. The mutations in the cell line may affect the expression or activity of this pathway, leading to changes in drug sensitivity.\n",
      "NOTCH1 and NOTCH3 are involved in cell fate determination and differentiation, and their mutations may affect the expression of ERK2. Therefore, VX-11E may be less effective in inhibiting ERK2 in cells with NOTCH1 or NOTCH3 mutations.\n",
      "PIK3R1 is a key regulator of the PI3K/AKT signaling pathway, which can interact with the ERK MAPK pathway. Mutations in PIK3R1 may affect the activity of this pathway, leading to changes in drug sensitivity.\n",
      "PPP2R1A is a protein phosphatase that regulates the activity of protein kinases, including ERK2. Mutations in PPP2R1A may affect the activity of ERK2, leading to changes in drug sensitivity.\n",
      "TP53 is a tumor suppressor gene that regulates cell cycle progression and apoptosis. Mutations in TP53 can lead to uncontrolled cell growth and resistance to drugs. Therefore, VX-11E may be more effective in inhibiting ERK2 in cells with TP53 mutations.\n",
      "TSC2 is a protein that regulates the activity of mTORC1, which is involved in cell growth and proliferation. Mutations in TSC2 may affect the activity of mTORC1, leading to changes in drug sensitivity.\n",
      "WHSC1L1 is a protein that regulates the activity of the ERK MAPK pathway. Mutations in WHSC1L1 may affect the activity of ERK2, leading to changes in drug sensitivity.\n",
      "In summary, the drug sensitivity of VX-11E on the cell line with the given mutations can be classified as follows:\n",
      "* NOTCH1: Resistant\n",
      "* NOTCH3: Resistant\n",
      "* PIK3R1: Sensitive\n",
      "* PPP2R1A: Sensitive\n",
      "* TP53: Sensitive\n",
      "* TSC2: Resistant\n",
      "* WHSC1L1: Sensitive\n",
      "\n",
      "Please note that this classification is based on the information provided and may not reflect the actual drug sensitivity of the cell line. Further experiments and data are needed to confirm these predictions.\n"
     ]
    }
   ],
   "source": [
    "### results from 7b_chat\n",
    "# pipe = pipeline(task=\"text-generation\", model=trainer.model, tokenizer=trainer.tokenizer, max_length=2000)\n",
    "\n",
    "# instruction: \"Think step by step and decide in a single word reflecting the drug sensitivity of the drug on the cell line: [Sensitive/Resistant]\"\n",
    "# prompt = \"Drug and cell line mutations: The drug is VX-11E. The drug SMILES structure is CC1=CN=C(NC2=C(Cl)C=C(F)C=C2)N=C1C3=CNC(C(N[C@@H](C4=CC(Cl)=CC=C4)CO)=O)=C3. Drug target is ERK2. Drug target pathway is ERK MAPK signaling.The mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.Drug Sensitivity: ?\"\n",
    "# prompt_content = f\"<s>[INST] <<SYS>>{instruction}<</SYS>>{prompt}[/INST]\"\n",
    "\n",
    "# # Run prompt and pipeline\n",
    "# result = pipe(prompt_content)\n",
    "# print(result[0]['generated_text']) # Not in one word???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
