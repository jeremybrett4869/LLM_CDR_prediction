{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00a559b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr  3 20:57:14 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 455.23.05    Driver Version: 455.23.05    CUDA Version: 11.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla P100-PCIE...  On   | 00000000:3B:00.0 Off |                    0 |\n",
      "| N/A   29C    P0    35W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla P100-PCIE...  On   | 00000000:5E:00.0 Off |                    0 |\n",
      "| N/A   28C    P0    37W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100S-PCI...  On   | 00000000:AF:00.0 Off |                    0 |\n",
      "| N/A   36C    P0    51W / 250W |  19788MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-PCIE...  On   | 00000000:D8:00.0 Off |                    0 |\n",
      "| N/A   35C    P0    39W / 250W |  14709MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    2   N/A  N/A     11486      C   ...da3/envs/base2/bin/python      729MiB |\n",
      "|    2   N/A  N/A     22652      C   ...da3/envs/base2/bin/python        0MiB |\n",
      "|    2   N/A  N/A     49685      C   ...s/sepsisrev/bin/python3.9    19051MiB |\n",
      "|    3   N/A  N/A      3013      C   ...envs/multitask/bin/python    13023MiB |\n",
      "|    3   N/A  N/A     11486      C   ...da3/envs/base2/bin/python      729MiB |\n",
      "|    3   N/A  N/A     22652      C   ...da3/envs/base2/bin/python      953MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7c5e93c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6a86ba9c0c4e0c8805a7d84cde5190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForCausalLM were not initialized from the model checkpoint at /data/yingfei/models/llm/llama2/llama/llama-2-7b-chat-hf and are newly initialized: ['model.layers.15.self_attn.rotary_emb.inv_freq', 'model.layers.4.self_attn.rotary_emb.inv_freq', 'model.layers.28.self_attn.rotary_emb.inv_freq', 'model.layers.8.self_attn.rotary_emb.inv_freq', 'model.layers.2.self_attn.rotary_emb.inv_freq', 'model.layers.29.self_attn.rotary_emb.inv_freq', 'model.layers.7.self_attn.rotary_emb.inv_freq', 'model.layers.3.self_attn.rotary_emb.inv_freq', 'model.layers.31.self_attn.rotary_emb.inv_freq', 'model.layers.17.self_attn.rotary_emb.inv_freq', 'model.layers.13.self_attn.rotary_emb.inv_freq', 'model.layers.12.self_attn.rotary_emb.inv_freq', 'model.layers.19.self_attn.rotary_emb.inv_freq', 'model.layers.21.self_attn.rotary_emb.inv_freq', 'model.layers.14.self_attn.rotary_emb.inv_freq', 'model.layers.11.self_attn.rotary_emb.inv_freq', 'model.layers.9.self_attn.rotary_emb.inv_freq', 'model.layers.27.self_attn.rotary_emb.inv_freq', 'model.layers.30.self_attn.rotary_emb.inv_freq', 'model.layers.1.self_attn.rotary_emb.inv_freq', 'model.layers.22.self_attn.rotary_emb.inv_freq', 'model.layers.10.self_attn.rotary_emb.inv_freq', 'model.layers.26.self_attn.rotary_emb.inv_freq', 'model.layers.24.self_attn.rotary_emb.inv_freq', 'model.layers.18.self_attn.rotary_emb.inv_freq', 'model.layers.16.self_attn.rotary_emb.inv_freq', 'model.layers.0.self_attn.rotary_emb.inv_freq', 'model.layers.5.self_attn.rotary_emb.inv_freq', 'model.layers.25.self_attn.rotary_emb.inv_freq', 'model.layers.23.self_attn.rotary_emb.inv_freq', 'model.layers.6.self_attn.rotary_emb.inv_freq', 'model.layers.20.self_attn.rotary_emb.inv_freq']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig, Trainer, pipeline\n",
    "from peft import LoraConfig\n",
    "from datasets import Dataset\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "from trl import SFTTrainer\n",
    "\n",
    "model_name = \"/data/yingfei/models/llm/llama2/llama/llama-2-7b-chat-hf\"\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=getattr(torch, \"float16\"),\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load llama2 model\n",
    "based_model = AutoModelForCausalLM.from_pretrained(model_name, \n",
    "  quantization_config=quant_config, \n",
    "  device_map={'':0}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80bc5cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(623, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACH-000001</td>\n",
       "      <td>Think step by step and decide the best drug op...</td>\n",
       "      <td>IMATINIB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACH-000002</td>\n",
       "      <td>Think step by step and decide the best drug op...</td>\n",
       "      <td>VX-11E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACH-000004</td>\n",
       "      <td>Think step by step and decide the best drug op...</td>\n",
       "      <td>CETUXIMAB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACH-000007</td>\n",
       "      <td>Think step by step and decide the best drug op...</td>\n",
       "      <td>AVAGACESTAT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACH-000008</td>\n",
       "      <td>Think step by step and decide the best drug op...</td>\n",
       "      <td>AZD5363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      cell_id                                             prompt       answer\n",
       "0  ACH-000001  Think step by step and decide the best drug op...     IMATINIB\n",
       "1  ACH-000002  Think step by step and decide the best drug op...       VX-11E\n",
       "2  ACH-000004  Think step by step and decide the best drug op...    CETUXIMAB\n",
       "3  ACH-000007  Think step by step and decide the best drug op...  AVAGACESTAT\n",
       "4  ACH-000008  Think step by step and decide the best drug op...      AZD5363"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data processing\n",
    "import pandas as pd\n",
    "\n",
    "train_prompt_data = pd.read_csv(\"/data/yingfei/cancer_data/llm_prompt_data/train_prompt_data_task2_simple.csv\")\n",
    "print(train_prompt_data.shape)\n",
    "train_prompt_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f4c04fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Think step by step and decide the best drug option for the cell line with given mutations: [Drug Name], [Reasoning].\n",
      "Drug 1: The drug is UNC1215. The drug SMILES structure is C1CCN(C1)C2CCN(CC2)C(=O)C3=CC(=C(C=C3)C(=O)N4CCC(CC4)N5CCCC5)NC6=CC=CC=C6. Drug target is L3MBTL3. Drug target pathway is Chromatin other.\n",
      "Drug 2: The drug is IMATINIB. The drug SMILES structure is CC1=C(C=C(C=C1)NC(=O)C2=CC=C(C=C2)CN3CCN(CC3)C)NC4=NC=CC(=N4)C5=CN=CC=C5. Drug target is ABL, KIT, PDGFR. Drug target pathway is Other, kinases.\n",
      "Drug 3: The drug is AT7867. The drug SMILES structure is C1CNCCC1(C2=CC=C(C=C2)C3=CNN=C3)C4=CC=C(C=C4)Cl. Drug target is AKT. Drug target pathway is PI3K/MTOR signaling.\n",
      "The mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.\n",
      "Best drug option: ?\n"
     ]
    }
   ],
   "source": [
    "print(train_prompt_data.prompt[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3b74d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Drug 1: The drug is UNC1215. The drug SMILES structure is C1CCN(C1)C2CCN(CC2)C(=O)C3=CC(=C(C=C3)C(=O)N4CCC(CC4)N5CCCC5)NC6=CC=CC=C6. Drug target is L3MBTL3. Drug target pathway is Chromatin other.Drug 2: The drug is IMATINIB. The drug SMILES structure is CC1=C(C=C(C=C1)NC(=O)C2=CC=C(C=C2)CN3CCN(CC3)C)NC4=NC=CC(=N4)C5=CN=CC=C5. Drug target is ABL, KIT, PDGFR. Drug target pathway is Other, kinases.Drug 3: The drug is AT7867. The drug SMILES structure is C1CNCCC1(C2=CC=C(C=C2)C3=CNN=C3)C4=CC=C(C=C4)Cl. Drug target is AKT. Drug target pathway is PI3K/MTOR signaling.The mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.Best drug option: ?', 'IMATINIB'), ('Drug 1: The drug is VX-11E. The drug SMILES structure is CC1=CN=C(NC2=C(Cl)C=C(F)C=C2)N=C1C3=CNC(C(N[C@@H](C4=CC(Cl)=CC=C4)CO)=O)=C3. Drug target is ERK2. Drug target pathway is ERK MAPK signaling.Drug 2: The drug is XMD8-92. The drug SMILES structure is CCOC1=C(C=CC(=C1)N2CCC(CC2)O)NC3=NC=C4C(=N3)N(C5=CC=CC=C5C(=O)N4C)C. Drug target is ERK5. Drug target pathway is ERK MAPK signaling.Drug 3: The drug is SABUTOCLAX. The drug SMILES structure is CC1=CC2=C(C(=C(C=C2C(=C1C3=C(C4=CC(=C(C(=C4C=C3C)C(=O)NC[C@H](C)C5=CC=CC=C5)O)O)O)O)O)O)C(=O)NC[C@H](C)C6=CC=CC=C6. Drug target is BCL2,  BCL-XL,  BFL1, MCL1. Drug target pathway is Apoptosis regulation.The mutations of the cell line are CDKN2A, CEBPA, KDM5C, NRAS, RPTOR.Best drug option: ?', 'VX-11E')]\n"
     ]
    }
   ],
   "source": [
    "instruction = \"Think step by step and decide the best drug option for the cell line with given mutations: [Drug Name], [Reasoning].\"\n",
    "\n",
    "q_a_lst = [] \n",
    "for i in range(len(train_prompt_data)):\n",
    "    question = train_prompt_data.loc[i, \"prompt\"].split(\"[Reasoning].\")[1].replace(\"\\n\", \"\")\n",
    "    answer = train_prompt_data.loc[i, \"answer\"]\n",
    "    q_a_lst.append((question, answer))\n",
    "print(q_a_lst[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7dfdf4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"instruction\", \"question\", \"answer\"], template=\"<s>[INST] <<SYS>>{instruction}<</SYS>>{question}[/INST]{answer}</s>\"\n",
    ")\n",
    "\n",
    "prompt_data = [prompt_template.format(instruction=instruction, question=q, answer=a) for q, a in q_a_lst[:10]]\n",
    "\n",
    "dataset = Dataset.from_dict({\"text\": prompt_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b2dde63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "based_model.config.use_cache = False\n",
    "based_model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0f6cef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/peft/utils/other.py:102: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:159: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0d8bdf0fa45439c8651a419cd6ef69f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:30: UserWarning: \n",
      "    There is an imbalance between your GPUs. You may want to exclude GPU 2 which\n",
      "    has less than 75% of the memory or cores of GPU 0. You can do so by setting\n",
      "    the device_ids argument to DataParallel, or by setting the CUDA_VISIBLE_DEVICES\n",
      "    environment variable.\n",
      "  warnings.warn(imbalance_warn.format(device_ids[min_pos], device_ids[max_pos]))\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/torch/nn/modules/container.py:691: UserWarning: nn.ParameterDict is being used with DataParallel but this is not supported. This dict will appear empty for the models replicated on each GPU except the original one.\n",
      "  warnings.warn(\"nn.ParameterDict is being used with DataParallel but this is not \"\n",
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/torch/nn/modules/container.py:597: UserWarning: Setting attributes on ParameterDict is not supported.\n",
      "  warnings.warn(\"Setting attributes on ParameterDict is not supported.\")\n",
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:06, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.590500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.530800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2, training_loss=2.5606207847595215, metrics={'train_runtime': 25.5428, 'train_samples_per_second': 0.783, 'train_steps_per_second': 0.078, 'total_flos': 187433617489920.0, 'train_loss': 2.5606207847595215, 'epoch': 2.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_params = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "training_params = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=1,\n",
    "    logging_steps=1,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=based_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_params,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=None,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_params,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78aa86b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/data/yingfei/models/llm/llama2/llama/llama_finetune/llama2_task2/tokenizer_config.json',\n",
       " '/data/yingfei/models/llm/llama2/llama/llama_finetune/llama2_task2/special_tokens_map.json',\n",
       " '/data/yingfei/models/llm/llama2/llama/llama_finetune/llama2_task2/tokenizer.model',\n",
       " '/data/yingfei/models/llm/llama2/llama/llama_finetune/llama2_task2/added_tokens.json',\n",
       " '/data/yingfei/models/llm/llama2/llama/llama_finetune/llama2_task2/tokenizer.json')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model = \"/data/yingfei/models/llm/llama2/llama/llama_finetune/llama2_task2\"\n",
    "\n",
    "trainer.model.save_pretrained(new_model)\n",
    "trainer.tokenizer.save_pretrained(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43ed6d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Think step by step and decide the best drug option for the cell line with given mutations: [Drug Name], [Reasoning].\n",
      "Drug 1: The drug is UNC1215. The drug SMILES structure is C1CCN(C1)C2CCN(CC2)C(=O)C3=CC(=C(C=C3)C(=O)N4CCC(CC4)N5CCCC5)NC6=CC=CC=C6. Drug target is L3MBTL3. Drug target pathway is Chromatin other.\n",
      "Drug 2: The drug is IMATINIB. The drug SMILES structure is CC1=C(C=C(C=C1)NC(=O)C2=CC=C(C=C2)CN3CCN(CC3)C)NC4=NC=CC(=N4)C5=CN=CC=C5. Drug target is ABL, KIT, PDGFR. Drug target pathway is Other, kinases.\n",
      "Drug 3: The drug is AT7867. The drug SMILES structure is C1CNCCC1(C2=CC=C(C=C2)C3=CNN=C3)C4=CC=C(C=C4)Cl. Drug target is AKT. Drug target pathway is PI3K/MTOR signaling.\n",
      "The mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.\n",
      "Best drug option: ?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cell_id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ACH-000052</td>\n",
       "      <td>Think step by step and decide the best drug op...</td>\n",
       "      <td>VX-11E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ACH-000053</td>\n",
       "      <td>Think step by step and decide the best drug op...</td>\n",
       "      <td>YK-4-279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ACH-000054</td>\n",
       "      <td>Think step by step and decide the best drug op...</td>\n",
       "      <td>GSK650394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ACH-000055</td>\n",
       "      <td>Think step by step and decide the best drug op...</td>\n",
       "      <td>FORETINIB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       cell_id                                             prompt     answer\n",
       "21  ACH-000052  Think step by step and decide the best drug op...     VX-11E\n",
       "22  ACH-000053  Think step by step and decide the best drug op...   YK-4-279\n",
       "23  ACH-000054  Think step by step and decide the best drug op...  GSK650394\n",
       "24  ACH-000055  Think step by step and decide the best drug op...  FORETINIB"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_prompt_data.prompt[0])\n",
    "train_prompt_data.iloc[21:25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "575a738e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING[XFORMERS]: xFormers can't load C++/CUDA extensions. xFormers was built for:\n",
      "    PyTorch 2.2.2+cu121 with CUDA 1201 (you have 1.10.1+cu111)\n",
      "    Python  3.9.19 (you have 3.9.12)\n",
      "  Please reinstall xformers (see https://github.com/facebookresearch/xformers#installing-xformers)\n",
      "  Memory-efficient attention, SwiGLU, sparse and more won't be available.\n",
      "  Set XFORMERS_MORE_DETAILS=1 for more details\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "The model 'PeftModelForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/transformers/generation/utils.py:1270: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation )\n",
      "  warnings.warn(\n",
      "/home/yingfei/llm_data/llm_env/lib/python3.9/site-packages/torch/utils/checkpoint.py:25: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\"None of the inputs have requires_grad=True. Gradients will be None\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>Think step by step and decide the best drug option for the cell line with given mutations: [Drug Name], [Reasoning].<</SYS>>Drug 1: The drug is UNC1215. The drug SMILES structure is C1CCN(C1)C2CCN(CC2)C(=O)C3=CC(=C(C=C3)C(=O)N4CCC(CC4)N5CCCC5)NC6=CC=CC=C6. Drug target is L3MBTL3. Drug target pathway is Chromatin other.Drug 2: The drug is IMATINIB. The drug SMILES structure is CC1=C(C=C(C=C1)NC(=O)C2=CC=C(C=C2)CN3CCN(CC3)C)NC4=NC=CC(=N4)C5=CN=CC=C5. Drug target is ABL, KIT, PDGFR. Drug target pathway is Other, kinases.Drug 3: The drug is AT7867. The drug SMILES structure is C1CNCCC1(C2=CC=C(C=C2)C3=CNN=C3)C4=CC=C(C=C4)Cl. Drug target is AKT. Drug target pathway is PI3K/MTOR signaling.The mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.Best drug option: ?[/INST]  Based on the information provided, the best drug option for the cell line with the given mutations would be Drug 3, AT7867.\n",
      "Reasoning:\n",
      "1. Drug target: AT7867 targets AKT, which is a key player in PI3K/MTOR signaling pathway, which is involved in cell growth and survival. The cell line has mutations in PIK3R1, TP53, and WHSC1L1, which are all involved in this pathway.\n",
      "2. Drug target pathway: AT7867 inhibits the PI3K/MTOR signaling pathway, which is the same pathway affected by the mutations in the cell line. This makes AT7867 a good option to target the mutations in the cell line.\n",
      "3. Drug-target interaction: The SMILES structure of AT7867 shows a good interaction with the AKT protein, which is the target of the drug. The drug binds to the ATP-binding site of AKT, which inhibits its activity.\n",
      "4. Efficacy: AT7867 has been shown to be effective in inhibiting the growth of cancer cells in various studies. It has also been shown to be effective in treating cancer in combination with other drugs.\n",
      "5. Toxicity: AT7867 has been shown to have low toxicity in clinical trials, which makes it a good option for cancer treatment.\n",
      "\n",
      "In conclusion, based on the information provided, the best drug option for the cell line with the given mutations would be AT7867. Its target of AKT, its interaction with the target protein, its efficacy, and low toxicity make it a good option for treating cancer cells with the specified mutations.\n"
     ]
    }
   ],
   "source": [
    "pipe = pipeline(task=\"text-generation\", model=trainer.model, tokenizer=trainer.tokenizer, max_length=2000)\n",
    "\n",
    "instruction: \"Think step by step and decide the best drug option for the cell line with given mutations: [Drug Name], [Reasoning].\"\n",
    "prompt = \"Drug 1: The drug is UNC1215. The drug SMILES structure is C1CCN(C1)C2CCN(CC2)C(=O)C3=CC(=C(C=C3)C(=O)N4CCC(CC4)N5CCCC5)NC6=CC=CC=C6. Drug target is L3MBTL3. Drug target pathway is Chromatin other.\\\n",
    "Drug 2: The drug is IMATINIB. The drug SMILES structure is CC1=C(C=C(C=C1)NC(=O)C2=CC=C(C=C2)CN3CCN(CC3)C)NC4=NC=CC(=N4)C5=CN=CC=C5. Drug target is ABL, KIT, PDGFR. Drug target pathway is Other, kinases.\\\n",
    "Drug 3: The drug is AT7867. The drug SMILES structure is C1CNCCC1(C2=CC=C(C=C2)C3=CNN=C3)C4=CC=C(C=C4)Cl. Drug target is AKT. Drug target pathway is PI3K/MTOR signaling.\\\n",
    "The mutations of the cell line are NOTCH1, NOTCH3, PIK3R1, PPP2R1A, TP53, TSC2, WHSC1L1.\\\n",
    "Best drug option: ?\"\n",
    "prompt_content = f\"<s>[INST] <<SYS>>{instruction}<</SYS>>{prompt}[/INST]\"\n",
    "\n",
    "# Run prompt and pipeline\n",
    "result = pipe(prompt_content)\n",
    "print(result[0]['generated_text']) # Not in one word???"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
